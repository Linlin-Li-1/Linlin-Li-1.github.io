<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.74.3 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="Linlin Li">
<meta name="keywords" content="Deep Learning, Machine Learning">
<meta name="description" content="In this post, I would like to share my implementation of several famous first-order optimization methods. I know that these methods have been implemented very well in many packages, but I hope my implementation can help you understand the ideas behind it.
Suppose we have \(N\) data examples and the parameters \(\mathbf{w} \in \mathcal{R}^D\).
For convenience, I first write a class named optimizer.
class optimizer: def __init__(self): pass def set_param(self, parameters): self.">


<meta property="og:description" content="In this post, I would like to share my implementation of several famous first-order optimization methods. I know that these methods have been implemented very well in many packages, but I hope my implementation can help you understand the ideas behind it.
Suppose we have \(N\) data examples and the parameters \(\mathbf{w} \in \mathcal{R}^D\).
For convenience, I first write a class named optimizer.
class optimizer: def __init__(self): pass def set_param(self, parameters): self.">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementation of First-order Optimization Methods (PyTorch)">
<meta name="twitter:title" content="Implementation of First-order Optimization Methods (PyTorch)">
<meta property="og:url" content="https://linlin-li-1.github.io/2020/11/implementation-of-first-order-optimization-methods-pytorch/">
<meta property="twitter:url" content="https://linlin-li-1.github.io/2020/11/implementation-of-first-order-optimization-methods-pytorch/">
<meta property="og:site_name" content="Linlin Li&#39;s Data Science Project Portfolio">
<meta property="og:description" content="In this post, I would like to share my implementation of several famous first-order optimization methods. I know that these methods have been implemented very well in many packages, but I hope my implementation can help you understand the ideas behind it.
Suppose we have \(N\) data examples and the parameters \(\mathbf{w} \in \mathcal{R}^D\).
For convenience, I first write a class named optimizer.
class optimizer: def __init__(self): pass def set_param(self, parameters): self.">
<meta name="twitter:description" content="In this post, I would like to share my implementation of several famous first-order optimization methods. I know that these methods have been implemented very well in many packages, but I hope my implementation can help you understand the ideas behind it.
Suppose we have \(N\) data examples and the parameters \(\mathbf{w} \in \mathcal{R}^D\).
For convenience, I first write a class named optimizer.
class optimizer: def __init__(self): pass def set_param(self, parameters): self.">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2020-11-04T23:30:38">
  
  
    <meta property="article:modified_time" content="2020-11-04T23:30:38">
  
  
  
    
      <meta property="article:section" content="Blog">
    
  
  
    
      <meta property="article:tag" content="Python">
    
      <meta property="article:tag" content="PyTorch Implementation">
    
  


<meta name="twitter:card" content="summary">







  <meta property="og:image" content="https://i.postimg.cc/Rh3mb67D/optim.png">
  <meta property="twitter:image" content="https://i.postimg.cc/Rh3mb67D/optim.png">





  <meta property="og:image" content="https://www.gravatar.com/avatar/1b1a0de53499fb41484c23d568bdbaea?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/1b1a0de53499fb41484c23d568bdbaea?s=640">


    <title>Implementation of First-order Optimization Methods (PyTorch)</title>

    <link rel="icon" href="https://linlin-li-1.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://linlin-li-1.github.io/2020/11/implementation-of-first-order-optimization-methods-pytorch/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://linlin-li-1.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://linlin-li-1.github.io/">Linlin Li&#39;s Data Science Project Portfolio</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://linlin-li-1.github.io/#about">
    
    
    
      
        <img class="header-picture" src="https://www.gravatar.com/avatar/1b1a0de53499fb41484c23d568bdbaea?s=90" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://linlin-li-1.github.io/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/1b1a0de53499fb41484c23d568bdbaea?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Linlin Li</h4>
        
          <h5 class="sidebar-profile-bio">Data Scientist @ <strong>DISH Network</strong> | Master&rsquo;s in Statistical Science @ <strong>Duke</strong></h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://linlin-li-1.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://linlin-li-1.github.io/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://linlin-li-1.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://linlin-li-1.github.io/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="mailto:lilinlin_ruc@outlook.com" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-envelope"></i>
      
      <span class="sidebar-button-desc">Email</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/Linlin-Li-1" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/linlin-li/" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-center">
  
    <h1 class="post-title" itemprop="headline">
      Implementation of First-order Optimization Methods (PyTorch)
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-11-04T23:30:38-05:00">
        
  November 4, 2020

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://linlin-li-1.github.io/categories/blog">Blog</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>In this post, I would like to share my implementation of several famous first-order optimization methods. I know that these methods have been implemented very well in many packages, but I hope my implementation can help you understand the ideas behind it.</p>

<p>Suppose we have <span  class="math">\(N\)</span> data examples and the parameters <span  class="math">\(\mathbf{w} \in \mathcal{R}^D\)</span>.</p>

<p>For convenience, I first write a class named <code>optimizer</code>.</p>
<pre><code>class optimizer:
    def __init__(self):
        pass
    def set_param(self, parameters):
        self.parameters = parameters
        return self
    def step(self):
        raise Exception(&#34;Not implemented&#34;)</code></pre>
<h2 id="1-stochastic-gradient-descent">1. (Stochastic) Gradient Descent</h2>

<p align="center">
  <img src="https://i.postimg.cc/T2KG7fsz/sgd.png" width="400" height="250" />
</p>

<p>Note that underlying i.i.d. assumption, <span  class="math">\(\mathbb{E}_{p(i_k)} [\nabla f_{i_k} (\mathbf{w}_k)] = \frac{1}{N} \sum^N_{i=1} \nabla f_{i} (\mathbf{w}_k)\)</span> . This means that <span  class="math">\(\frac{1}{N} \sum^N_{i=1} \nabla f_{i} (\mathbf{w}_k)\)</span> is an unbiased estimator of <span  class="math">\(\mathbb{E}_{p(i_k)} [\nabla f_{i_k} (\mathbf{w}_k)]\)</span>.</p>

<p>The performance of Stochastic Gradient Descent (SGD) and Gradient Descent (GD) are shown in the picture below. Even though gradient descent algorithms may take a much longer time, its direction is more accurate.
<p align="center">
  <img src="https://i.postimg.cc/CKYBVz8r/gd.png" width="300" height="300" />
</p></p>

<p>However, this iterative algorithms are expensive:</p>

<ul>
<li>Gradient descent takes:

<ul>
<li><span  class="math">\(\mathcal{O}(N)\)</span> to estimate gradients</li>
<li><span  class="math">\(\mathcal{O}(1)\)</span> to update the parameters</li>
</ul></li>
<li>Stochastic gradient descent takes:

<ul>
<li><span  class="math">\(\mathcal{O}(1)\)</span> to estimate gradients</li>
<li><span  class="math">\(\mathcal{O}(1)\)</span> to update the parameters</li>
</ul></li>
<li>Newton's method (classical optimization approach) takes:

<ul>
<li><span  class="math">\(\mathcal{O}(N)\)</span> to estimate gradients and <span  class="math">\(\mathcal{O}(ND^2)\)</span> to estimate Hessian</li>
<li><span  class="math">\(\mathcal{O}(D^3)\)</span> to update the parameters</li>
</ul></li>
</ul>

<p>It is easy to see that for GoogLeNet image classifier on the ImageNet dataset, with <span  class="math">\(N=10^6\)</span> and <span  class="math">\(D \cong 5\times 10^6\)</span>, GD is nearly impossible.</p>
<pre><code>class SGD(optimizer):
    def __init__(self, lr):
        self.lr = lr
    def step(self):
        for param in self.parameters:
            param.data -= self.lr * param.grad.data</code></pre>
<p>The limitations of the stochastic gradient method are:</p>

<ul>
<li>constants (and hence step sizes) are usually unknown</li>
<li>variance in the gradient estimation limits convergence (much of the recent literature discuss ways of minimizing variance)</li>
<li>does not utilize the curvature in space, can be very poorly conditioned.</li>
</ul>

<h2 id="2-momentum-method">2. Momentum method</h2>

<p>Classically, momentum &quot;dampens&quot; the highly oscillatory terms.
<p align="center">
  <img src="https://i.postimg.cc/VLsBMskC/momentum.png" width="400" height="200" />
</p></p>
<pre><code>class SGD_Momentum(optimizer):
    def __init__(self, lr, beta):
        self.lr = lr
        self.beta = beta
        self.m = None
    def step(self):
        if self.m is None:
            self.m = [torch.zeros_like(param) for param in self.parameters]
        for i, param in enumerate(self.parameters):
            mm = self.beta * self.m[i] + param.grad.data
            self.m[i] = mm
            param.data -= self.lr * mm</code></pre>
<h2 id="3-nesterovs-accelerated-gradient-nag-method">3. Nesterov’s accelerated gradient (NAG) method</h2>

<p>Nesterov methods invented originally by Yurii Nesterov is used for acceleration of the first order methods.</p>

<p>In addition to the gradient information from the previous iteration, gradients from other iteration(s) are contributed with appropriate weights to the calculation of the current estimate of the parameter.</p>

<p align="center">
  <img src="https://i.postimg.cc/J7pKpHgq/NAG.png" width="500" height="400" />
</p>
<pre><code>class SGD_NAG(optimizer):
    def __init__(self, lr, beta):
        self.lr = lr
        self.beta = beta
        self.v = self.t = None
    def step(self):
        if self.v is None:
            self.v, self.t = zip(*[(torch.zeros_like(param), param.data) for param in self.parameters])
            self.v, self.t = list(self.v), list(self.t)
        for i, param in enumerate(self.parameters):
            m = 1 - self.v[i]
            self.v[i] = (1 + torch.sqrt(1 + 4 * self.v[i]**2)) / 2
            mm = m / self.v[i]
            t1 = self.t[i]
            self.t[i] = param.data - 1 / self.beta * param.grad.data
            param.data = (1 - mm) * t1 + mm * t1</code></pre>
<h2 id="4--rmsprop-algorithm">4.  RMSprop Algorithm</h2>

<p>Note that RMSprop algorithm does not improve the convergence rate, but it can improve optimizations constants. Since SGD methods are very sensitive to the step size sequences, adaptive methods like RMSprop is robust to the step size sequences, so we can use the same settings across many models and datasets.</p>

<p>The key idea is that sparsely occurring features may be very informative - only decrease step size when information relating to that parameter is seen. This is provably improves regret bounds compared to online (stochastic) gradient descent.</p>

<p align="center">
  <img src="https://i.postimg.cc/wvZcLZ0s/rms.png" width="500" height="230" />
</p>

<p>RMSprop algorithm will converge if <span  class="math">\(\alpha \rightarrow 0\)</span> with at rate <span  class="math">\(t^{1/2}\)</span>. The typical step size to use is <span  class="math">\(\alpha \cong 10^{-3}\)</span>.</p>
<pre><code>                
class SGD_RMSprop(optimizer):
    def __init__(self, lr, beta, eps, r):
        self.lr = lr
        self.beta = beta
        self.eps = eps
        self.r = r
        self.m = None
    def step(self):
        if self.m is None:
            self.m = [torch.zeros_like(param) for param in self.parameters]
        for i, param in enumerate(self.parameters):
            self.m[i] = self.beta * self.m[i] + (1-self.beta) * torch.square(param.grad.data)
            eta = self.r / (torch.sqrt(self.m[i]) + self.eps)
            param.data -= self.lr * eta * param.grad.data</code></pre>
<p>The limitations of RMSprop:</p>

<ul>
<li>there is no convergence guarantee</li>
<li>no inclusion of momentum</li>
</ul>

<p>In practive, RMSprop has been largely replaced by the ADAM optimizer, which can address these issues.</p>

<h2 id="5--adam-algorithm">5.  ADAM Algorithm</h2>

<p>ADAM was inspired by RMSprop, if <span  class="math">\(\beta_1=0\)</span>, then they are very similar algorithms. And ADAM has a provale regret bound under a decreasing step size. The typical parameters for ADAM are
<span  class="math">\(
\alpha = 10^{-3}, ~\beta_1 = 0.9, ~\beta_2 = 0.999. Note that \)</span>\beta_1<span  class="math">\( going higher will reduce the variance of gradients, but can add bias. 
\)</span>
<p align="center">
  <img src="https://i.postimg.cc/kDrs3zSd/adam.png" width="400" height="320" />
</p></p>
<pre><code>class SGD_Adam(optimizer):
    def __init__(self, lr, betas, eps):
        self.lr = lr
        self.beta1 = betas[0]
        self.beta2 = betas[1]
        self.eps = eps 
        self.m = self.v = None
        self.t = None
    def step(self):
        if self.t is None:
            self.t = 0
        if self.m is None:
            self.v, self.m = zip(*[(torch.zeros_like(param), torch.zeros_like(param)) for param in self.parameters])
            self.v, self.m = list(self.v), list(self.m)
        for i, param in enumerate(self.parameters):
            self.t = self.t + 1
            self.m[i] = self.beta1 * self.m[i] + (1-self.beta1) * param.grad.data
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * torch.square(param.grad.data)
            mc = self.m[i] / (1 - np.power(self.beta1, self.t))
            vc = self.v[i] / (1 - np.power(self.beta2, self.t))
            param.data -= self.lr * mc / (torch.sqrt(vc) + self.eps)</code></pre>
<h2 id="6--comparison">6.  Comparison</h2>

<p align="center">
  <img src="https://i.postimg.cc/Nj2yrFZb/compare0.png" width="400" height="400" />
</p>

<p align="center">
  <img src="https://i.postimg.cc/T3KbqC5V/compare.png" width="450" height="450" />
</p>

<ul>
<li>Stochastic gradient descent methods are effective at quickly obtaining a reasonable solution.</li>
<li>Stochastic gradient descent will converge to the optimum if given enough time.

<ul>
<li>will converge to a fixed point in nonconvex problems as well.</li>
</ul></li>
<li>Adding in adaptive higher order information can reduce parameter tuning and account for curvature information.</li>
<li>Many tuning parameters within these algorithms

<ul>
<li>hopefully their meaning and how to set them is clearer now.</li>
</ul></li>
</ul>

<p>Thanks for reading.</p>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://linlin-li-1.github.io/tags/python/">Python</a>

  <a class="tag tag--primary tag--small" href="https://linlin-li-1.github.io/tags/pytorch-implementation/">PyTorch Implementation</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://linlin-li-1.github.io/2020/11/covid-19-analysis/" data-tooltip="COVID-19 Analysis">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://linlin-li-1.github.io/2020/10/image-classification-using-cnn-pytorch/" data-tooltip="Image Classification using CNN (PyTorch)">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2021 Linlin Li. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://linlin-li-1.github.io/2020/11/covid-19-analysis/" data-tooltip="COVID-19 Analysis">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://linlin-li-1.github.io/2020/10/image-classification-using-cnn-pytorch/" data-tooltip="Image Classification using CNN (PyTorch)">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/1b1a0de53499fb41484c23d568bdbaea?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Linlin Li</h4>
    
      <div id="about-card-bio">Data Scientist @ <strong>DISH Network</strong> | Master&rsquo;s in Statistical Science @ <strong>Duke</strong></div>
    
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Denver, CO
      </div>
    
  </div>
</div>

    

    
  
    <div id="cover" style="background-image:url('https://i.postimg.cc/xCkJPyMv/blue.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://linlin-li-1.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
  




    
  </body>
</html>

